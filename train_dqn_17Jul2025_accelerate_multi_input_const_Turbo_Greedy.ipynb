{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13dad8a-cf2e-469d-adbc-ecbe46d11f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.turbo_wrapper_greedy import TGraphWrapper\n",
    "\n",
    "from CybORG import CybORG\n",
    "from CybORG.Simulator.Scenarios.EnterpriseScenarioGenerator import EnterpriseScenarioGenerator\n",
    "from CybORG.Agents import SleepAgent, EnterpriseGreenAgent, FiniteStateRedAgent\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from submission import Submission\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import traceback\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f02e4b-7ecd-4411-af6a-2ec31bb4996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim_1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim_2, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01baecf-8bf0-4147-a97f-3540ec661162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, agent, input_dim, hidden_dim_1, output_dim, lr, memory_capacity):\n",
    "        self.device = device\n",
    "        self.policy_net = DQN(input_dim, hidden_dim_1, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, hidden_dim_1, output_dim).to(self.device)\n",
    "        self.name = agent\n",
    "        print(f\"agent {self.name} initiation\")\n",
    "        try:\n",
    "            # raise Exception\n",
    "            self.policy_net.load_state_dict(torch.load(f\"dqn_Turbo_Greedy_policy_{agent}_net_Jul25.pth\"))\n",
    "            self.target_net.load_state_dict(torch.load(f\"dqn_Turbo_Greedy_target_{agent}_net_Jul25.pth\"))\n",
    "            self.policy_net.train()\n",
    "            self.target_net.eval()\n",
    "            print(\"model loaded successfully\")\n",
    "        except Exception:\n",
    "            # traceback.print_exc()\n",
    "            print(\"model not loaded\")\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            self.target_net.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        # self.criterion = nn.MSELoss()\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "        self.replay_buffer = deque(maxlen=memory_capacity)\n",
    "        \n",
    "# \"\"\" Does this need to be edited? \"\"\"\n",
    "    def select_action(self, state, epsilon):\n",
    "        if state[-1] == 0: #since this action is None\n",
    "            return 0\n",
    "        else:\n",
    "            print(state[-1])\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, 6)  # 4 + 2 + 1 actions\n",
    "        with torch.no_grad():\n",
    "            q = self.policy_net(state.to(self.device))\n",
    "            return q.argmax().item()\n",
    "            \n",
    "    def store_transition(self, *transition): \n",
    "        (state, deception, reward, next_state, done) = transition\n",
    "        i = f\"blue_agent_{self.name}\"\n",
    "        agent_transition = (state[self.name], deception[self.name], reward[i], next_state[self.name], done[self.name])\n",
    "        self.replay_buffer.append(agent_transition)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return random.sample(self.replay_buffer, batch_size)\n",
    "\n",
    "    def update(self, batch, gamma):\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.cat(states).to(self.device)\n",
    "        next_states = torch.cat(next_states).to(self.device)\n",
    "        actions = torch.tensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target = -rewards + gamma * next_q * (1-dones) \n",
    "\n",
    "        loss = self.criterion(q_values, target)\n",
    "        # loss = nn.SmoothL1Loss()(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # print(f\"loss is ({loss})\")\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfebde6-f584-4aac-8292-a4b88dc712dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Initializing with parallelization parameters\n",
    "HYPER_PARAMS = SimpleNamespace(\n",
    "    agent_num = 5,\n",
    "    episode_length = 500,\n",
    "    episode_num = 2000,\n",
    "    workers = 1,  # Number of parallel environments\n",
    "    batch_size = 512,\n",
    "    memory_capacity = 30000,\n",
    "    lr = 0.0008,\n",
    "    tau = 0.0001,\n",
    "    target_update_freq = 5,\n",
    "    epsilon = 0.1,\n",
    "    gamma = 0\n",
    ")\n",
    "\n",
    "MAX_THREADS = min(os.cpu_count(), 30)  # Limit to CPU count or 20 (default was 12)\n",
    "torch.set_num_threads(MAX_THREADS)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b16b0d-c686-4a19-9dbe-5f7ea6b8a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env initialized\n",
      "in init: 0\n",
      "multiple environments created.\n",
      "agent 0 initiation\n",
      "model not loaded\n",
      "Agent 0 Created!\n",
      "agent 1 initiation\n",
      "model not loaded\n",
      "Agent 1 Created!\n",
      "agent 2 initiation\n",
      "model not loaded\n",
      "Agent 2 Created!\n",
      "agent 3 initiation\n",
      "model not loaded\n",
      "Agent 3 Created!\n",
      "agent 4 initiation\n",
      "model not loaded\n",
      "Agent 4 Created!\n",
      "[<__main__.DQNAgent object at 0x15309c1984d0>, <__main__.DQNAgent object at 0x15309c199bb0>, <__main__.DQNAgent object at 0x15304c508620>, <__main__.DQNAgent object at 0x15304c5089b0>, <__main__.DQNAgent object at 0x15304c508d40>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create environments in parallel\n",
    "def create_env(seed):\n",
    "    sg = EnterpriseScenarioGenerator(\n",
    "        blue_agent_class=SleepAgent,\n",
    "        green_agent_class=EnterpriseGreenAgent,\n",
    "        red_agent_class=FiniteStateRedAgent,\n",
    "        steps=HYPER_PARAMS.episode_length,\n",
    "    )\n",
    "    cyborg = CybORG(sg, \"sim\", seed=seed)\n",
    "    return TGraphWrapper(cyborg)\n",
    "\n",
    "# Create multiple environments\n",
    "envs = Parallel(n_jobs=HYPER_PARAMS.workers)(\n",
    "    delayed(create_env)(seed + i) for i in range(HYPER_PARAMS.workers)\n",
    ")\n",
    "print(\"multiple environments created.\")\n",
    "# Create shared DQN agent\n",
    "agents = []\n",
    "for ag_num in range(HYPER_PARAMS.agent_num):\n",
    "    if ag_num < HYPER_PARAMS.agent_num - 1:\n",
    "        agent = DQNAgent(ag_num, input_dim=258, hidden_dim_1=512, output_dim=7, \n",
    "                         lr=HYPER_PARAMS.lr, memory_capacity=HYPER_PARAMS.memory_capacity)\n",
    "    else:\n",
    "        agent = DQNAgent(ag_num, input_dim=494, hidden_dim_1=1024, output_dim=7, \n",
    "                 lr=HYPER_PARAMS.lr, memory_capacity=HYPER_PARAMS.memory_capacity) \n",
    "    agents.append(agent)\n",
    "    print(f\"Agent {ag_num} Created!\")\n",
    "print(agents)\n",
    "# Function to collect experiences from one environmen\n",
    "@torch.no_grad()\n",
    "def collect_experiences(env_idx, episode, hp):\n",
    "    # Set thread limits for this process\n",
    "    torch.set_num_threads(MAX_THREADS // hp.workers)\n",
    "    \n",
    "    env = envs[env_idx]\n",
    "    transitions = []\n",
    "\n",
    "    states = []\n",
    "    obs_all, _ = env.reset()\n",
    "    for agent, ag_num in zip(agents, range(HYPER_PARAMS.agent_num)):\n",
    "        agent_name = f\"blue_agent_{ag_num}\"\n",
    "        dict_obs = env.env.environment_controller.get_last_observation(agent_name).data\n",
    "        state = env.encode_dict_obs_expnd_inpt(dict_obs, dict_obs, env.gen_obs[agent_name], env.manp_obs[agent_name]).to('cpu')  # Keep on CPU when collecting\n",
    "        states.append(state)\n",
    "    \n",
    "    done = np.array([False, False, False, False, False])\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    epsilon_dyn = max((1 - ((1-hp.epsilon)*episode / (hp.episode_num/2))), hp.epsilon)\n",
    "    \n",
    "    while not done.any() and step_count < hp.episode_length:\n",
    "        # Get action (on CPU to avoid CUDA overhead for small operations)\n",
    "        deceptions = []\n",
    "        for agent in agents:\n",
    "            q_vals = agent.policy_net(states[agent.name].to(device))\n",
    "            if env.M > 0:\n",
    "                if random.random() < epsilon_dyn:\n",
    "                    deception = random.randint(0, 6) #4 + 2 + 1 actions\n",
    "                else:\n",
    "                    deception = q_vals.argmax().item()\n",
    "            else:\n",
    "                deception = 0 #no action\n",
    "            deceptions.append(deception)\n",
    "            \n",
    "        # Execute in environment\n",
    "        obs_all, manp_dict_obs, reward, term, _, _, _, obs, manp_obs = env.modified_step(deceptions, obs_all)\n",
    "        next_state = []; done = []\n",
    "        for agent in agents:\n",
    "            agent_name = f\"blue_agent_{agent.name}\"\n",
    "            next_dict_obs = env.env.environment_controller.get_last_observation(agent_name).data\n",
    "            next_state.append(env.encode_dict_obs_expnd_inpt(next_dict_obs, manp_dict_obs[agent_name], obs[agent_name], manp_obs[agent_name]).to('cpu'))\n",
    "            done.append(term[f\"blue_agent_{agent.name}\"])\n",
    "        # print(next_state)\n",
    "        done = np.array(done)\n",
    "        \"\"\"whose reward?\"\"\"\n",
    "        # Store transition\n",
    "        transitions.append((states, deceptions, reward, next_state, done))\n",
    "        \n",
    "        states = next_state\n",
    "        total_reward += (reward['blue_agent_0'] + reward['blue_agent_1'] + reward['blue_agent_2'] + reward['blue_agent_3'] + reward['blue_agent_4'])/5  \n",
    "        # print(f\"reward blue agent 0: {reward['blue_agent_0']} \\n reward blue agent 1: {reward['blue_agent_1']} \\nreward blue agent 2: {reward['blue_agent_2']} \\nreward blue agent 3: {reward['blue_agent_3']} \\n reward blue agent 4: {reward['blue_agent_4']} \")\n",
    "        # print()\n",
    "        step_count += 1\n",
    "    \"\"\"ADD THIS AS AN EDIT!!!\"\"\"\n",
    "    env.M = 500\n",
    "    return transitions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170350eb-f79e-4f6f-b6b8-ff48aaa09f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:32<17:52:06, 32.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/2000 [12:47<20:27:21, 37.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18594790119297638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 41/2000 [25:25<20:38:09, 37.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1935249860797609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 61/2000 [38:07<20:41:54, 38.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19758994786106807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 81/2000 [50:46<19:51:29, 37.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21452846910272322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 101/2000 [1:03:04<19:21:09, 36.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21902220370818165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 118/2000 [1:13:44<20:00:52, 38.29s/it]"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "all_loss = np.zeros(HYPER_PARAMS.episode_num)\n",
    "all_rewards = np.zeros(HYPER_PARAMS.episode_num)\n",
    "\n",
    "for episode in tqdm(range(0, HYPER_PARAMS.episode_num, HYPER_PARAMS.workers)):\n",
    "    # Collect experiences in parallel\n",
    "    results = Parallel(prefer='processes', n_jobs=HYPER_PARAMS.workers)(\n",
    "        delayed(collect_experiences)(i % len(envs), episode + i, HYPER_PARAMS) \n",
    "        for i in range(min(HYPER_PARAMS.workers, HYPER_PARAMS.episode_num - episode))\n",
    "    )\n",
    "    \n",
    "    # Process collected experiences\n",
    "    all_transitions = []\n",
    "    for i, (transitions, reward) in enumerate(results):\n",
    "        if episode + i < HYPER_PARAMS.episode_num:\n",
    "            all_rewards[episode + i] = reward\n",
    "            all_transitions.extend(transitions)\n",
    "    \n",
    "    # print(len(all_transitions))\n",
    "    # Add all collected transitions to replay buffer\n",
    "    for transition in all_transitions:\n",
    "        for agent in agents:\n",
    "            agent.store_transition(*transition)\n",
    "    for agent in agents:\n",
    "        # Perform learning updates if buffer has enough samples\n",
    "        # print(len(agent.replay_buffer))\n",
    "        if len(agent.replay_buffer) >= HYPER_PARAMS.batch_size:\n",
    "            # Perform multiple updates\n",
    "            update_count = len(all_transitions) // 10  # Adjust this ratio as needed\n",
    "            # print(all_transitions)\n",
    "            # print()\n",
    "            for _ in range(update_count):\n",
    "                batch = agent.sample_batch(HYPER_PARAMS.batch_size)\n",
    "                loss = agent.update(batch, gamma=HYPER_PARAMS.gamma)\n",
    "                avg_loss = loss / update_count\n",
    "                for i in range(min(HYPER_PARAMS.workers, HYPER_PARAMS.episode_num - episode)):\n",
    "                    if episode + i < HYPER_PARAMS.episode_num:\n",
    "                        all_loss[episode + i] += avg_loss/5 #editted\n",
    "        \n",
    "        # Soft update target network\n",
    "        with torch.no_grad():\n",
    "            for target_param, policy_param in zip(agent.target_net.parameters(), agent.policy_net.parameters()):\n",
    "                target_param.data.copy_((1 - HYPER_PARAMS.tau) * target_param.data + HYPER_PARAMS.tau * policy_param.data)\n",
    "        \n",
    "    # Save checkpoint occasionally\n",
    "    if episode % 20 == 0:\n",
    "        # torch.save(agent.policy_net.state_dict(), f\"dqn_policy_{agent.name}_net_checkpoint_{episode}.pth\")\n",
    "        np.save('dqn_training_loss_Jul25_Turbo_Greedy.npy', all_loss[:episode])\n",
    "        print(all_loss[episode])\n",
    "        np.save('dqn_training_rewards_Jul25_Turbo_Greedy.npy', all_rewards[:episode])\n",
    "\n",
    "# Save final model\n",
    "for agent in agents:\n",
    "    torch.save(agent.policy_net.state_dict(), f'dqn_Turbo_Greedy_policy_{agent.name}_net_Jul25.pth')\n",
    "    torch.save(agent.target_net.state_dict(), f'dqn_Turbo_Greedy_target_{agent.name}_net_Jul25.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de54ce-86a4-4178-9015-3e7b9023861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in agents:\n",
    "    torch.save(agent.policy_net.state_dict(), f'dqn_Turbo_Greedy_policy_{agent.name}_net_Jul25.pth')\n",
    "    torch.save(agent.target_net.state_dict(), f'dqn_Turbo_Greedy_target_{agent.name}_net_Jul25.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff2d35-7dfd-4883-a078-fbdd0178cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = np.load(\"dqn_training_loss_Jul25_Turbo_Greedy.npy\")\n",
    "print(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541efee-a76c-4755-8239-d8914677cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47763075-d036-4b09-8fac-27e7e76d6fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb = np.load(\"dqn_training_rewards_Jul25_Turbo_Greedy.npy\")\n",
    "print(bbb)\n",
    "plt.plot(bbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfafa7-0c89-419d-860c-f02faea65a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 9\n",
    "H = 16\n",
    "i = 1\n",
    "l = 1 + (3*S + 2*H)*i + 4*8\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972a2ea-5a4f-45ed-be90-3deb9a507c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 9\n",
    "H = 16\n",
    "i = 3\n",
    "l = 1 + (3*S + 2*H)*i + 4*8\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a3eb2-9c4e-423e-a510-4b8236127b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
